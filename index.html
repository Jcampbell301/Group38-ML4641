<!DOCTYPE html>
<html>
<head>
<style>
body {background-color: powderblue;}
p    {margin: 25px 50px 25px 50px;}
</style>
</head>
<body>

<h1>
CS 4641 - Wildfire Susceptibility Mapping
</h1>

<span>John Campbell, </span><span>Justin Lu, </span><span>Brian Maxey, </span><span>Isaac Tomblin, and </span><span>George Tomecek</span>

<h3>Background</h3>
<p>In an era of climate change, wildfires have become a focal point of study - particularly for the West Coast. Trends for recent years have not shown much change in the total amount of fires, but they have shown an increase in the amount of land burned by these fires (Wildfire Statistics). The implications can be severe: deforestation, homelessness, and increased carbon emissions, among others.</p>

<h3>Problem Definition</h3>
<p>While it is unlikely that wildfires are preventable, with appropriate action, the damages done can be reduced. To do this, there needs to be proactive action in conjunction with better indicators for wildfire risk. Our project sought to identify these variables and created two predictive models to classify the susceptibility of a state to wildfires. Note that in the scope of this work, susceptibility is directly related to the total number of fires in a specific location. Consequently, the variables were used to create a predictive model that predicts the number of wildfires in a state. Ideally, these results yield meaningful data regarding wildfire severities and frequencies which can be used to schedule and allocate fire prevention resources more effectively.</p>
 
<h3>Data Collection</h3>
 <p>To collect data, major physical and environmental factors were brainstormed and analyzed using intuition as well as research into existing wildfire papers. Specifically, Chas-Amil M.L’s paper, Natural and Social Factors Influencing Forest Fire Occurrence at a Local Spatial Scale, was a particularly helpful source for determining several factors that were connected to wildfires<sup>[1]</sup>. Analysis of the paper led to the decision that datasets for the following factors should be collected: temperature, precipitation, humidity, biomass, wind, and human factors.</p>
 
 <p>The first three factors, temperature, precipitation, and humidity were collected for each of the mainland United States on a monthly basis for the timeframe of 2011 to 2020<sup>[2]</sup>. Alaska and Hawaii were excluded due to their significant spatial disconnect from the rest of the states in addition to having distinct habitats (tundra and tropical island respectively). When considering a mechanism to capture biomass data, forest coverage was chosen<sup>[3]</sup>. This metric was used on a per state basis and assumed to be consistent across the timespan. The assumption of consistency is reasonable because at the level of state with only a 10 year range there is likely relatively minimal changes to the amount of forestland in each respective state. Wind datasets were found; however, there was a large price to obtain these datasets. Due to budgetary constraints, wind was ultimately excluded from the model. Finally, human factors is a relatively broad term. As such, the population of each state for each year was selected to represent this feature<sup>[4]</sup>. This parameter is a decent estimate given that statewide data was scarce for other “human factors”. </p> 
 
 <p><b>Table 1</b>, shown below, details each variable used and provides a brief description. Note that some variables were derived from a single dataset. For example, max temperature was calculated on a yearly basis from the same data set that average temperature was calculated.</p>
 
 
<h3>Methods</h3> 
<h4>Correlation Matrix</h4>
 <p> </p>
 
<h4>PCA</h4>
 <p>Due to the plethora of variables that can go into the creation of forest fires previously stated, dimensionality reduction using PCA was implemented to shrink the number of dimensions in the model, thus simplifying it, while still maintaining effectiveness (Chas-Amil et al). 
  The fit and transformation of the dataset was calculated using the PCA class from the skLearn.decompossion module, and reduced the data down to eight principle components. The explained variance for each principal component was then found and summed across the components to ensure 95% of the explained variance was accounted for in the model. This new dataset was then used in different regression models to test the accuracy of the model. 
  Afterwards, an unsupervised learning model to cluster the susceptibility levels of counties in our study area.</p>
 
 <h4>Naive Bayes</h4>
 <p> </p>
 
 <h4>Linear Regression</h4>
 <p></p>
 <h4>Logistic Regression</h4>
 <p></p>


<h3>Results</h3>
<p>Our evaluation will lead to a label of ranges across the study area depicting the susceptibility levels of wildfires in respective counties. These levels will then be clustered to help identify larger trends among the variety of societal and environmental factors we intend on surveying. Once these larger trends are established, we will look into the primary reason that these regions are overwhelmingly susceptible and attempt to identify appropriate action to reduce these susceptibility index scores in the future.</p>
 <h4>PCA</h4>
  <p>The goal when running PCA was to reduce the number of inputs while still accurately representing the dataset. In order to maintain the integrity of the data, 95% of the explained variance needed to be accounted for after the reduction took place.</p>
  <img src="ExplainedVariance.png" alt="Principal Component Graph" height = "500" width = "650" class="center">
  <p>This above graph led to the conclusion that it was possible to safely reduce four dimensions and still maintain the integrity of the data. 
   Below is a matrix showing how each principal component contributed to the total expected variance.</p>
  <img src="ExplainedVarMatrix.png" alt="Explained Variance Matrix" height = "150" width = "663" class="center">
 
 <h4>Naive Bayes</h4>
 <p></p>
 
 <h4>Linear Regression</h4>
 <p></p>
 
 <h4>Logistic Regression</h4>
 <p></p>
  
<h3>Discussion</h3>
<p>One challenge that this project will have to work around is potential inconsistency in datasets due to the nature of climate science. For example, a measurement as basic as temperature can have a large amount of variance for a location depending on how it is measured. If the measurement occurs in a shady area, over concrete, during a windy period, etc., then that can affect the reading. Additionally, the time range of datasets might prove to be a source of inconsistency. Technology is constantly developing. The instruments used to measure each of the factors that might be considered for wildfires have likely changed in some capacity over time. This will likely further contribute to a lack of precision for any datasets used. While there may be obstacles present, the potential outcome is meaningful: having a means to predict wildfire impact can save property, the climate, and ultimately, lives. </p>

<h3>References</h3>
<p>Chas-Amil, Maria Luisa; Touza, Julia M.; Prestemon, Jeffrey P.; McClean, Colin J. 2012. Natural and social factors influencing forest fire occurrence at a local spatial scale. In: Spano, Donatella; Bacciu, Valentina; Salis, Michele; Sirca, Costatino (eds.). Modelling Fire Behavior and Risk. Global Fire Monitoring Center: Freiburg, Germany, 181-186.</p>
<p>Climate at a glance. (2021, February). Retrieved March 01, 2021, from https://www.ncdc.noaa.gov/cag/county/mapping.</p>
<p>Wildfire Statistics. (2021, January 04). Retrieved March 01, 2021, from https://crsreports.congress.gov/.</p>

<iframe width="1048" height="655" src="https://www.youtube.com/embed/ZqiDOShWvDw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


</body>
</html>
